{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "import chainer_chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "import numpy as np\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "from chainer_chemistry import datasets\n",
    "from chainer_chemistry.dataset.preprocessors.ggnn_preprocessor import GGNNPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2786.30it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    load data\n",
    "\"\"\"\n",
    "dataset, dataset_smiles = datasets.get_qm9(GGNNPreprocessor(kekulize=True), return_smiles=True,\n",
    "                                           target_index=np.random.choice(range(133000), 100, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3287.95it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    load data\n",
    "\"\"\"\n",
    "dataset2, dataset_smiles2 = datasets.get_qm9(GGNNPreprocessor(kekulize=True), return_smiles=True,\n",
    "                                           target_index=np.random.choice(range(133000), 1000, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 9\n",
    "atom_types = [6, 8, 7, 9, 1]\n",
    "\n",
    "def adj(x):\n",
    "    x = x[1]\n",
    "    adjacency = np.zeros((V, V)).astype(float)\n",
    "    adjacency[:len(x[0]), :len(x[0])] = x[0] + 2 * x[1] + 3 * x[2]\n",
    "    return torch.tensor(adjacency)\n",
    "\n",
    "\n",
    "def sig(x):\n",
    "    x = x[0]\n",
    "    atoms = np.ones((V)).astype(float)\n",
    "    atoms[:len(x)] = x\n",
    "    out = np.array([int(atom == atom_type) for atom_type in atom_types for atom in atoms]).astype(float)\n",
    "    return torch.tensor(out).reshape(5, len(atoms)).T\n",
    "\n",
    "\n",
    "def target(x):\n",
    "    x = x[2]\n",
    "    return torch.tensor(x)\n",
    "\n",
    "\n",
    "adjs = torch.stack(list(map(adj, dataset)))\n",
    "sigs = torch.stack(list(map(sig, dataset)))\n",
    "prop = torch.stack(list(map(target, dataset)))[:, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GCN:\n",
    "    \"\"\"\n",
    "        Graph convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # -- initialize weight\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # self.W = torch.empty(in_features, out_features, dtype=torch.float64)\n",
    "        # nn.init.xavier_normal_(self.W)\n",
    "        # self.W = self.W.requires_grad_()\n",
    "        self.W = nn.Parameter(torch.DoubleTensor(in_features, out_features))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "\n",
    "        # -- non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def __call__(self, A, H):\n",
    "        # -- GCN propagation rule\n",
    "        A += torch.eye(A.shape[0], dtype=torch.float64)\n",
    "        D = torch.diag(1. / torch.sqrt(torch.sum(A, axis=1)))\n",
    "        # D2 = torch.diag(torch.sqrt(torch.sum(A, axis=1)))\n",
    "        H = self.relu(D @ A @ D @ H @ self.W)\n",
    "        return H\n",
    "\n",
    "\n",
    "class GraphPooling:\n",
    "    \"\"\"\n",
    "        Graph pooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, H):\n",
    "        # -- multi-set pooling operator\n",
    "        # This pooling layer should sum the node features as graph features\n",
    "        h = torch.sum(H,dim=0)\n",
    "        return h\n",
    "\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Regression  model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # -- initialize layers\n",
    "        self.GCN = GCN(5,16)\n",
    "        self.pooling = GraphPooling()\n",
    "        self.linear = nn.Linear(16,1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, A, h0):\n",
    "        # A:  N x N\n",
    "        # h0: N x in\n",
    "        # output: N x out\n",
    "        H0=self.GCN(A,h0)\n",
    "        H1=self.pooling(H0)\n",
    "        H2=self.linear(H1)\n",
    "        return H2\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64 torch.float64 torch.float64 tensor(-0.2732)\n",
      "MyModel(\n",
      "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "pred = model.forward(adjs[0],sigs[0])\n",
    "print(adjs[0].dtype,sigs[0].dtype,pred.dtype,prop[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1  loss: 9.12081319\n",
      "epoch:   6  loss: 0.01892377\n",
      "epoch:  11  loss: 0.26137610\n",
      "epoch:  16  loss: 0.05453241\n",
      "epoch:  21  loss: 0.00108972\n",
      "epoch:  26  loss: 0.00000478\n",
      "epoch:  31  loss: 0.00001484\n",
      "epoch:  36  loss: 0.00105452\n",
      "epoch:  41  loss: 0.05414137\n",
      "epoch:  46  loss: 0.00033467\n",
      "epoch:  51  loss: 0.00000969\n",
      "epoch:  56  loss: 0.00124499\n",
      "epoch:  61  loss: 0.00451642\n",
      "epoch:  66  loss: 0.00028055\n",
      "epoch:  71  loss: 0.00031269\n",
      "epoch:  76  loss: 0.00302445\n",
      "epoch:  81  loss: 0.00013801\n",
      "epoch:  86  loss: 0.00139607\n",
      "epoch:  91  loss: 0.00002021\n",
      "epoch:  96  loss: 0.00001260\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "MyLoss = []\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.SGD([{'params': model.GCN.W},\n",
    "                {'params': model.linear.parameters()}], lr=0.1)\n",
    "# -- update parameters\n",
    "for epoch in range(1):\n",
    "    for i in range(100):\n",
    "\n",
    "        # -- predict\n",
    "        pred = model.forward(adjs[i], sigs[i])\n",
    "\n",
    "        # -- loss\n",
    "        loss = criterion(pred, prop[i].to(torch.float64)) # RMSE\n",
    "        MyLoss.append(loss)\n",
    "        if i%5 == 1:\n",
    "            print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # -- optimize\n",
    "\n",
    "# -- plot loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "with torch.no_grad():\n",
    "    plt.plot(range(len(MyLoss)),MyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([-0.2049], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2547999918460846\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2529999911785126\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24150000512599945\n",
      "pred: tensor([-0.2254], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2320999950170517\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24770000576972961\n",
      "pred: tensor([-0.2454], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25110000371932983\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.30000001192092896\n",
      "pred: tensor([-0.2358], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2409999966621399\n",
      "pred: tensor([-0.2240], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23669999837875366\n",
      "pred: tensor([-0.2424], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20749999582767487\n",
      "pred: tensor([-0.2424], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23669999837875366\n",
      "pred: tensor([-0.2455], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22540000081062317\n",
      "pred: tensor([-1.7997], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23180000483989716\n",
      "pred: tensor([-0.2351], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24799999594688416\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2046000063419342\n",
      "pred: tensor([-0.2608], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19429999589920044\n",
      "pred: tensor([-0.2433], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2639999985694885\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2806999981403351\n",
      "pred: tensor([-0.2528], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2702000141143799\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23729999363422394\n",
      "pred: tensor([-0.2316], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.218299999833107\n",
      "pred: tensor([-0.2343], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25600001215934753\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20600000023841858\n",
      "pred: tensor([-0.2434], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.243599995970726\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2379000037908554\n",
      "pred: tensor([-0.2450], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2623000144958496\n",
      "pred: tensor([-0.2386], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2581000030040741\n",
      "pred: tensor([-0.2203], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2734000086784363\n",
      "pred: tensor([-0.2432], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2362000048160553\n",
      "pred: tensor([-0.2096], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25519999861717224\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24570000171661377\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21889999508857727\n",
      "pred: tensor([-0.2167], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23319999873638153\n",
      "pred: tensor([-0.2273], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2635999917984009\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.271699994802475\n",
      "pred: tensor([-0.2316], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2703000009059906\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22499999403953552\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23669999837875366\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2328999936580658\n",
      "pred: tensor([-0.2418], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2290000021457672\n",
      "pred: tensor([-0.2430], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24120000004768372\n",
      "pred: tensor([-0.2360], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2289000004529953\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2320999950170517\n",
      "pred: tensor([-0.2358], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2371000051498413\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23899999260902405\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22100000083446503\n",
      "pred: tensor([-0.2593], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2583000063896179\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23340000212192535\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23839999735355377\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2556000053882599\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2134999930858612\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.262800008058548\n",
      "pred: tensor([-0.2435], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24789999425411224\n",
      "pred: tensor([-0.2443], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22450000047683716\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24089999496936798\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24160000681877136\n",
      "pred: tensor([-0.2091], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2540999948978424\n",
      "pred: tensor([-0.2149], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.249099999666214\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23739999532699585\n",
      "pred: tensor([-0.2234], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22920000553131104\n",
      "pred: tensor([-0.2435], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.258899986743927\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24709999561309814\n",
      "pred: tensor([-0.2282], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23430000245571136\n",
      "pred: tensor([-0.2125], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2639000117778778\n",
      "pred: tensor([-0.2219], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23880000412464142\n",
      "pred: tensor([-0.2567], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2282000035047531\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23469999432563782\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22920000553131104\n",
      "pred: tensor([-0.2252], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24210000038146973\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26260000467300415\n",
      "pred: tensor([-0.2606], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2101999968290329\n",
      "pred: tensor([-0.2229], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2443999946117401\n",
      "pred: tensor([-0.2412], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2535000145435333\n",
      "pred: tensor([-0.2012], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2460000067949295\n",
      "pred: tensor([-0.2440], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2257000058889389\n",
      "pred: tensor([-0.2443], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25589999556541443\n",
      "pred: tensor([-0.2194], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2433999925851822\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21619999408721924\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2370000034570694\n",
      "pred: tensor([-0.2410], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24699999392032623\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24799999594688416\n",
      "pred: tensor([-0.2409], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21879999339580536\n",
      "pred: tensor([-0.2576], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24879999458789825\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22529999911785126\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23929999768733978\n",
      "pred: tensor([-0.2357], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23639999330043793\n",
      "pred: tensor([-0.2442], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27410000562667847\n",
      "pred: tensor([-0.2356], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23600000143051147\n",
      "pred: tensor([-0.2393], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22339999675750732\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23389999568462372\n",
      "pred: tensor([-0.2354], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26100000739097595\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2597000002861023\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22349999845027924\n",
      "pred: tensor([-0.2299], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23880000412464142\n",
      "pred: tensor([-0.2422], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2849000096321106\n",
      "pred: tensor([-0.2324], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2484000027179718\n",
      "pred: tensor([-0.2592], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24210000038146973\n",
      "pred: tensor([-0.2405], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2533000111579895\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2531999945640564\n",
      "pred: tensor([-0.2416], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21950000524520874\n",
      "pred: tensor([-0.2348], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.28600001335144043\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22619999945163727\n",
      "pred: tensor([-0.2222], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24400000274181366\n",
      "pred: tensor([-0.2140], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2538999915122986\n",
      "pred: tensor([-0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23070000112056732\n",
      "pred: tensor([-0.2799], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2554999887943268\n",
      "pred: tensor([-0.2209], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2705000042915344\n",
      "pred: tensor([-0.2226], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22010000050067902\n",
      "pred: tensor([-0.2330], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25220000743865967\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2184000015258789\n",
      "pred: tensor([-0.2537], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22460000216960907\n",
      "pred: tensor([-0.2265], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2590000033378601\n",
      "pred: tensor([-0.2410], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24869999289512634\n",
      "pred: tensor([-0.2360], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24699999392032623\n",
      "pred: tensor([-0.2162], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2522999942302704\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23199999332427979\n",
      "pred: tensor([-0.2353], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24860000610351562\n",
      "pred: tensor([-0.2182], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24969999492168427\n",
      "pred: tensor([-0.2392], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23389999568462372\n",
      "pred: tensor([-0.2424], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25380000472068787\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20520000159740448\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2630999982357025\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23839999735355377\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2483000010251999\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22660000622272491\n",
      "pred: tensor([-0.2404], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2303999960422516\n",
      "pred: tensor([-0.2590], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25360000133514404\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2524000108242035\n",
      "pred: tensor([-0.2281], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24770000576972961\n",
      "pred: tensor([-0.2243], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2524999976158142\n",
      "pred: tensor([-0.2469], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21469999849796295\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.290800005197525\n",
      "pred: tensor([-0.2249], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2433999925851822\n",
      "pred: tensor([-0.2423], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2428], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2467000037431717\n",
      "pred: tensor([-0.2275], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2572999894618988\n",
      "pred: tensor([-0.2393], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24220000207424164\n",
      "pred: tensor([-0.2282], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25380000472068787\n",
      "pred: tensor([-0.2374], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.266400009393692\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27489998936653137\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25279998779296875\n",
      "pred: tensor([-0.2366], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20170000195503235\n",
      "pred: tensor([-0.2300], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24160000681877136\n",
      "pred: tensor([-0.2336], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23149999976158142\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24410000443458557\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23559999465942383\n",
      "pred: tensor([-0.2374], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24369999766349792\n",
      "pred: tensor([-0.2233], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2393999993801117\n",
      "pred: tensor([-0.2596], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2483000010251999\n",
      "pred: tensor([-0.2353], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22450000047683716\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21639999747276306\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23430000245571136\n",
      "pred: tensor([-0.2547], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2337999939918518\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24310000240802765\n",
      "pred: tensor([-0.2533], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22499999403953552\n",
      "pred: tensor([-0.2288], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24699999392032623\n",
      "pred: tensor([-0.2234], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2303999960422516\n",
      "pred: tensor([-0.2192], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451000064611435\n",
      "pred: tensor([-0.2753], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29170000553131104\n",
      "pred: tensor([-0.2240], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.250900000333786\n",
      "pred: tensor([-0.2239], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24879999458789825\n",
      "pred: tensor([-0.2420], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2572999894618988\n",
      "pred: tensor([-0.2565], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2418999969959259\n",
      "pred: tensor([-0.2396], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24009999632835388\n",
      "pred: tensor([-0.2455], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2198999971151352\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22200000286102295\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18479999899864197\n",
      "pred: tensor([-0.2236], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23409999907016754\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26460000872612\n",
      "pred: tensor([-0.2327], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2770000100135803\n",
      "pred: tensor([-0.2606], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25600001215934753\n",
      "pred: tensor([-0.2230], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24130000174045563\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23010000586509705\n",
      "pred: tensor([-0.2202], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26499998569488525\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2214999943971634\n",
      "pred: tensor([-0.2211], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24250000715255737\n",
      "pred: tensor([-0.2353], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24770000576972961\n",
      "pred: tensor([-0.2699], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22130000591278076\n",
      "pred: tensor([-0.2133], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2624000012874603\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25290000438690186\n",
      "pred: tensor([-0.2182], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2303999960422516\n",
      "pred: tensor([-0.2379], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2594], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2621000111103058\n",
      "pred: tensor([-0.2451], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2475000023841858\n",
      "pred: tensor([-0.2042], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22540000081062317\n",
      "pred: tensor([-0.2209], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2379000037908554\n",
      "pred: tensor([-0.2089], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2597000002861023\n",
      "pred: tensor([-0.2487], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2143000066280365\n",
      "pred: tensor([-0.2336], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2632000148296356\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2630999982357025\n",
      "pred: tensor([-0.2119], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23270000517368317\n",
      "pred: tensor([-0.2308], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22380000352859497\n",
      "pred: tensor([-0.2442], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23759999871253967\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23759999871253967\n",
      "pred: tensor([-0.2449], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22089999914169312\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21359999477863312\n",
      "pred: tensor([-0.2330], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2271], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27140000462532043\n",
      "pred: tensor([-0.2533], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2515000104904175\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2282000035047531\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2296999990940094\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.1793999969959259\n",
      "pred: tensor([-0.2415], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2151000052690506\n",
      "pred: tensor([-0.2481], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24629999697208405\n",
      "pred: tensor([-0.2537], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.3059999942779541\n",
      "pred: tensor([-0.1923], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.225600004196167\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22439999878406525\n",
      "pred: tensor([-0.2109], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2614000141620636\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21160000562667847\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.3237000107765198\n",
      "pred: tensor([-0.2608], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21639999747276306\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19050000607967377\n",
      "pred: tensor([-0.2593], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23800000548362732\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24480000138282776\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20389999449253082\n",
      "pred: tensor([-0.2644], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23119999468326569\n",
      "pred: tensor([-0.2222], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23029999434947968\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24809999763965607\n",
      "pred: tensor([-0.2245], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21610000729560852\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24619999527931213\n",
      "pred: tensor([-0.2321], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23970000445842743\n",
      "pred: tensor([-0.2031], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25119999051094055\n",
      "pred: tensor([-0.2799], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2328999936580658\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22830000519752502\n",
      "pred: tensor([-0.2361], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26759999990463257\n",
      "pred: tensor([-0.2409], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20749999582767487\n",
      "pred: tensor([-0.2212], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21979999542236328\n",
      "pred: tensor([-0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23190000653266907\n",
      "pred: tensor([-0.2392], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2287999987602234\n",
      "pred: tensor([-0.2534], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.188400000333786\n",
      "pred: tensor([-0.2304], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23579999804496765\n",
      "pred: tensor([-0.2371], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24009999632835388\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23229999840259552\n",
      "pred: tensor([-0.2420], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22920000553131104\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2313999980688095\n",
      "pred: tensor([-0.2404], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2757999897003174\n",
      "pred: tensor([-0.2153], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.259799987077713\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23469999432563782\n",
      "pred: tensor([-0.2210], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24480000138282776\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18950000405311584\n",
      "pred: tensor([-0.2313], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22460000216960907\n",
      "pred: tensor([-0.2601], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2542000114917755\n",
      "pred: tensor([-0.2205], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24079999327659607\n",
      "pred: tensor([-0.2262], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22010000050067902\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23149999976158142\n",
      "pred: tensor([-0.2218], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2281000018119812\n",
      "pred: tensor([-0.2316], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24250000715255737\n",
      "pred: tensor([-0.2443], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.271699994802475\n",
      "pred: tensor([-0.2354], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23770000040531158\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2815999984741211\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20149999856948853\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24459999799728394\n",
      "pred: tensor([-0.2355], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23649999499320984\n",
      "pred: tensor([-0.2352], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22380000352859497\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2433999925851822\n",
      "pred: tensor([-0.2187], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2345000058412552\n",
      "pred: tensor([-0.2384], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451000064611435\n",
      "pred: tensor([-0.2168], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2621000111103058\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2134000062942505\n",
      "pred: tensor([-0.2144], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2402999997138977\n",
      "pred: tensor([-0.9005], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2468000054359436\n",
      "pred: tensor([-0.2337], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2320999950170517\n",
      "pred: tensor([-0.2595], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2223999947309494\n",
      "pred: tensor([-0.2312], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24560000002384186\n",
      "pred: tensor([-0.9128], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.211899995803833\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23589999973773956\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2378000020980835\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21809999644756317\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21660000085830688\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2160000056028366\n",
      "pred: tensor([-0.2594], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23890000581741333\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2061000019311905\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23589999973773956\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2442999929189682\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23559999465942383\n",
      "pred: tensor([-0.2127], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2533999979496002\n",
      "pred: tensor([-0.2341], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26159998774528503\n",
      "pred: tensor([-0.2533], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20659999549388885\n",
      "pred: tensor([-0.2510], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19539999961853027\n",
      "pred: tensor([-0.2443], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25270000100135803\n",
      "pred: tensor([-0.2415], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.28119999170303345\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24799999594688416\n",
      "pred: tensor([-0.2209], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2362000048160553\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24210000038146973\n",
      "pred: tensor([-0.2398], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2264000028371811\n",
      "pred: tensor([-0.2537], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22499999403953552\n",
      "pred: tensor([-0.2243], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2345999926328659\n",
      "pred: tensor([-0.2497], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24869999289512634\n",
      "pred: tensor([-0.2086], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24060000479221344\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2409999966621399\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2337000072002411\n",
      "pred: tensor([-0.2340], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24240000545978546\n",
      "pred: tensor([-0.2263], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26269999146461487\n",
      "pred: tensor([-0.2143], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24300000071525574\n",
      "pred: tensor([-0.2432], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2556999921798706\n",
      "pred: tensor([-0.2320], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23770000040531158\n",
      "pred: tensor([-0.2282], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.241799995303154\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25839999318122864\n",
      "pred: tensor([-0.2154], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2425999939441681\n",
      "pred: tensor([-0.2389], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23510000109672546\n",
      "pred: tensor([-0.2448], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24539999663829803\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24889999628067017\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22920000553131104\n",
      "pred: tensor([-0.2455], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2232999950647354\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24140000343322754\n",
      "pred: tensor([-0.2366], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23440000414848328\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2827000021934509\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22310000658035278\n",
      "pred: tensor([-0.2235], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2572999894618988\n",
      "pred: tensor([-0.2298], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25600001215934753\n",
      "pred: tensor([-0.2445], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25279998779296875\n",
      "pred: tensor([-0.2745], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26499998569488525\n",
      "pred: tensor([-0.2241], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25\n",
      "pred: tensor([-0.2288], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24950000643730164\n",
      "pred: tensor([-0.2759], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2653000056743622\n",
      "pred: tensor([-0.2170], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25209999084472656\n",
      "pred: tensor([-0.2440], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25929999351501465\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24699999392032623\n",
      "pred: tensor([-0.2526], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2533999979496002\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2547000050544739\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2249000072479248\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24410000443458557\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2304999977350235\n",
      "pred: tensor([-0.2144], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24660000205039978\n",
      "pred: tensor([-0.2423], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24089999496936798\n",
      "pred: tensor([-0.2210], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23690000176429749\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24879999458789825\n",
      "pred: tensor([-0.2206], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24330000579357147\n",
      "pred: tensor([-0.2589], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2533000111579895\n",
      "pred: tensor([-1.1385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20579999685287476\n",
      "pred: tensor([-0.2523], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20180000364780426\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26109999418258667\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24050000309944153\n",
      "pred: tensor([-0.2552], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2409999966621399\n",
      "pred: tensor([-0.2554], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.295199990272522\n",
      "pred: tensor([-0.2432], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23980000615119934\n",
      "pred: tensor([-0.2377], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2842000126838684\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2102999985218048\n",
      "pred: tensor([-0.2536], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2750000059604645\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2498999983072281\n",
      "pred: tensor([-0.2274], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22669999301433563\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23800000548362732\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21320000290870667\n",
      "pred: tensor([-0.2565], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24560000002384186\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22040000557899475\n",
      "pred: tensor([-0.2271], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22519999742507935\n",
      "pred: tensor([-0.2218], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24809999763965607\n",
      "pred: tensor([-0.2593], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2655999958515167\n",
      "pred: tensor([-0.2249], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23149999976158142\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2206999957561493\n",
      "pred: tensor([-0.2399], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24459999799728394\n",
      "pred: tensor([-0.2321], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24300000071525574\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2623000144958496\n",
      "pred: tensor([-0.2428], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23160000145435333\n",
      "pred: tensor([-0.2366], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2160000056028366\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2759999930858612\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24369999766349792\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29660001397132874\n",
      "pred: tensor([-0.2256], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2603999972343445\n",
      "pred: tensor([-0.2548], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27709999680519104\n",
      "pred: tensor([-0.2286], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2362000048160553\n",
      "pred: tensor([-0.2431], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23010000586509705\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2671000063419342\n",
      "pred: tensor([-0.2450], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.1923000067472458\n",
      "pred: tensor([-0.2189], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2298000007867813\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24240000545978546\n",
      "pred: tensor([-0.2413], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2621999979019165\n",
      "pred: tensor([-0.2433], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25519999861717224\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2599000036716461\n",
      "pred: tensor([-0.2533], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2092999964952469\n",
      "pred: tensor([-0.2225], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2409999966621399\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22679999470710754\n",
      "pred: tensor([-0.2179], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24220000207424164\n",
      "pred: tensor([-0.2214], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.31200000643730164\n",
      "pred: tensor([-0.2397], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20309999585151672\n",
      "pred: tensor([-0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22179999947547913\n",
      "pred: tensor([-0.2337], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21799999475479126\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2410999983549118\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2321999967098236\n",
      "pred: tensor([-0.2274], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2524000108242035\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2240999937057495\n",
      "pred: tensor([-0.2219], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22830000519752502\n",
      "pred: tensor([-0.2604], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.17159999907016754\n",
      "pred: tensor([-0.2569], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26109999418258667\n",
      "pred: tensor([-0.2445], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.251800000667572\n",
      "pred: tensor([-0.2467], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.226500004529953\n",
      "pred: tensor([-0.2220], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23749999701976776\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24220000207424164\n",
      "pred: tensor([-0.2217], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2419], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23849999904632568\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24150000512599945\n",
      "pred: tensor([-0.9005], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2483000010251999\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24969999492168427\n",
      "pred: tensor([-0.2358], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23690000176429749\n",
      "pred: tensor([-0.2226], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25679999589920044\n",
      "pred: tensor([-0.2245], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23919999599456787\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2402999997138977\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20260000228881836\n",
      "pred: tensor([-0.2237], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451999932527542\n",
      "pred: tensor([-0.2438], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25440001487731934\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2597000002861023\n",
      "pred: tensor([-0.2472], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22529999911785126\n",
      "pred: tensor([-0.2265], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2565999925136566\n",
      "pred: tensor([-0.1632], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25999999046325684\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24310000240802765\n",
      "pred: tensor([-0.2496], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2554999887943268\n",
      "pred: tensor([-0.2425], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24169999361038208\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22220000624656677\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2240000069141388\n",
      "pred: tensor([-0.2570], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2833999991416931\n",
      "pred: tensor([-0.2534], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25450000166893005\n",
      "pred: tensor([-0.2323], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24740000069141388\n",
      "pred: tensor([-0.2592], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23759999871253967\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20180000364780426\n",
      "pred: tensor([-0.2531], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2547999918460846\n",
      "pred: tensor([-0.2384], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.226500004529953\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25760000944137573\n",
      "pred: tensor([-0.2195], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2621000111103058\n",
      "pred: tensor([-0.2375], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22709999978542328\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24969999492168427\n",
      "pred: tensor([-0.2551], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24619999527931213\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24799999594688416\n",
      "pred: tensor([-0.2389], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18850000202655792\n",
      "pred: tensor([-0.2448], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24819999933242798\n",
      "pred: tensor([-0.2388], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2535000145435333\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22450000047683716\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21709999442100525\n",
      "pred: tensor([-0.2495], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24809999763965607\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26159998774528503\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2298000007867813\n",
      "pred: tensor([-0.2203], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18619999289512634\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21969999372959137\n",
      "pred: tensor([-0.2348], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25699999928474426\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24899999797344208\n",
      "pred: tensor([-0.2415], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2524000108242035\n",
      "pred: tensor([-0.2427], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23019999265670776\n",
      "pred: tensor([-0.2186], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2401999980211258\n",
      "pred: tensor([-0.2472], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2362000048160553\n",
      "pred: tensor([-0.2377], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2702000141143799\n",
      "pred: tensor([-0.2366], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2651999890804291\n",
      "pred: tensor([-0.2596], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22540000081062317\n",
      "pred: tensor([-0.2384], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.195700004696846\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2492000013589859\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26809999346733093\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25600001215934753\n",
      "pred: tensor([-0.2356], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25029999017715454\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21359999477863312\n",
      "pred: tensor([-0.2197], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23330000042915344\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2159000039100647\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21780000627040863\n",
      "pred: tensor([-0.2537], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22439999878406525\n",
      "pred: tensor([-0.2455], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2395000010728836\n",
      "pred: tensor([-0.2068], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2574999928474426\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23100000619888306\n",
      "pred: tensor([-0.2594], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26840001344680786\n",
      "pred: tensor([-0.1939], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29159998893737793\n",
      "pred: tensor([-0.2410], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26570001244544983\n",
      "pred: tensor([-0.2337], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23190000653266907\n",
      "pred: tensor([-0.2420], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2515000104904175\n",
      "pred: tensor([-0.2282], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25290000438690186\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23160000145435333\n",
      "pred: tensor([-0.2415], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24529999494552612\n",
      "pred: tensor([-0.2379], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27469998598098755\n",
      "pred: tensor([-0.2434], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18870000541210175\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2484000027179718\n",
      "pred: tensor([-0.2175], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22859999537467957\n",
      "pred: tensor([-0.2497], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2897999882698059\n",
      "pred: tensor([-0.2223], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23270000517368317\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23489999771118164\n",
      "pred: tensor([-0.2436], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2540999948978424\n",
      "pred: tensor([-0.2314], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2603999972343445\n",
      "pred: tensor([-0.2183], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23469999432563782\n",
      "pred: tensor([-0.2271], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26010000705718994\n",
      "pred: tensor([-0.2098], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2606000006198883\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24490000307559967\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2303999960422516\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23160000145435333\n",
      "pred: tensor([-0.2725], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2694000005722046\n",
      "pred: tensor([-0.2272], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.28450000286102295\n",
      "pred: tensor([-0.2141], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2386000007390976\n",
      "pred: tensor([-0.2218], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22990000247955322\n",
      "pred: tensor([-0.2388], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2134999930858612\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2483000010251999\n",
      "pred: tensor([-0.2694], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26179999113082886\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24320000410079956\n",
      "pred: tensor([-0.2481], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.15559999644756317\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2597000002861023\n",
      "pred: tensor([-0.2218], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2685000002384186\n",
      "pred: tensor([-0.2566], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.1965000033378601\n",
      "pred: tensor([-0.2304], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24269999563694\n",
      "pred: tensor([-0.2502], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2727000117301941\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21960000693798065\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24950000643730164\n",
      "pred: tensor([-0.2253], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26579999923706055\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22499999403953552\n",
      "pred: tensor([-0.2592], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20659999549388885\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21770000457763672\n",
      "pred: tensor([-0.2221], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2522999942302704\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2709999978542328\n",
      "pred: tensor([-0.1863], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2442999929189682\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.17990000545978546\n",
      "pred: tensor([-0.2358], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24619999527931213\n",
      "pred: tensor([-0.2381], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2712000012397766\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24320000410079956\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23810000717639923\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2312999963760376\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.263700008392334\n",
      "pred: tensor([-0.2589], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26109999418258667\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2151000052690506\n",
      "pred: tensor([-0.2437], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21729999780654907\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18039999902248383\n",
      "pred: tensor([-0.2297], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.16760000586509705\n",
      "pred: tensor([-0.2296], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24770000576972961\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21969999372959137\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25040000677108765\n",
      "pred: tensor([-0.2180], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25\n",
      "pred: tensor([-0.2619], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.28700000047683716\n",
      "pred: tensor([-0.2523], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24950000643730164\n",
      "pred: tensor([-0.2371], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21330000460147858\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22450000047683716\n",
      "pred: tensor([-0.2316], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22840000689029694\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19020000100135803\n",
      "pred: tensor([-0.2212], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2370000034570694\n",
      "pred: tensor([-0.2371], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24060000479221344\n",
      "pred: tensor([-0.2376], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23409999907016754\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23639999330043793\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21649999916553497\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19859999418258667\n",
      "pred: tensor([-0.2331], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23749999701976776\n",
      "pred: tensor([-0.2135], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2093999981880188\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19370000064373016\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.273499995470047\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23090000450611115\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2773999869823456\n",
      "pred: tensor([-0.1878], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25200000405311584\n",
      "pred: tensor([-0.2292], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2563000023365021\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23810000717639923\n",
      "pred: tensor([-0.2218], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23569999635219574\n",
      "pred: tensor([-0.2251], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2500999867916107\n",
      "pred: tensor([-0.2415], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2784000039100647\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25839999318122864\n",
      "pred: tensor([-0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20970000326633453\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23070000112056732\n",
      "pred: tensor([-0.2287], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24269999563694\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2353000044822693\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451000064611435\n",
      "pred: tensor([-0.2489], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2485000044107437\n",
      "pred: tensor([-0.2337], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24709999561309814\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2467000037431717\n",
      "pred: tensor([-0.2458], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2793000042438507\n",
      "pred: tensor([-0.2554], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2460000067949295\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22750000655651093\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21230000257492065\n",
      "pred: tensor([-0.2416], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23589999973773956\n",
      "pred: tensor([-0.2509], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24320000410079956\n",
      "pred: tensor([-0.2386], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23690000176429749\n",
      "pred: tensor([-0.2216], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25850000977516174\n",
      "pred: tensor([-0.2409], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22139999270439148\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23510000109672546\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23240000009536743\n",
      "pred: tensor([-0.2435], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24500000476837158\n",
      "pred: tensor([-0.2208], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24660000205039978\n",
      "pred: tensor([-0.2270], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23720000684261322\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29019999504089355\n",
      "pred: tensor([-0.2808], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2590999901294708\n",
      "pred: tensor([-0.2344], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2526000142097473\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2190999984741211\n",
      "pred: tensor([-0.2424], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2401999980211258\n",
      "pred: tensor([-0.2410], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23399999737739563\n",
      "pred: tensor([-0.2430], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2401999980211258\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20679999887943268\n",
      "pred: tensor([-0.2600], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24169999361038208\n",
      "pred: tensor([-0.2336], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20509999990463257\n",
      "pred: tensor([-0.2392], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23829999566078186\n",
      "pred: tensor([-0.2374], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24729999899864197\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22179999947547913\n",
      "pred: tensor([-0.2298], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25040000677108765\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24269999563694\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2402999997138977\n",
      "pred: tensor([-0.2350], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2354000061750412\n",
      "pred: tensor([-0.2594], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24009999632835388\n",
      "pred: tensor([-0.2562], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.296999990940094\n",
      "pred: tensor([-0.2574], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2442999929189682\n",
      "pred: tensor([-0.2387], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23579999804496765\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22220000624656677\n",
      "pred: tensor([-0.2372], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23160000145435333\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23980000615119934\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21770000457763672\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21789999306201935\n",
      "pred: tensor([-0.2483], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23260000348091125\n",
      "pred: tensor([-0.2275], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23899999260902405\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23479999601840973\n",
      "pred: tensor([-0.2404], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19140000641345978\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26100000739097595\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21629999577999115\n",
      "pred: tensor([-0.2518], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26440000534057617\n",
      "pred: tensor([-0.2386], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23729999363422394\n",
      "pred: tensor([-0.2215], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24070000648498535\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19189999997615814\n",
      "pred: tensor([-0.2071], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24379999935626984\n",
      "pred: tensor([-0.2375], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23810000717639923\n",
      "pred: tensor([-0.2432], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23409999907016754\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20239999890327454\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21960000693798065\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25529998540878296\n",
      "pred: tensor([-0.2454], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25850000977516174\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23250000178813934\n",
      "pred: tensor([-0.2407], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23749999701976776\n",
      "pred: tensor([-0.2755], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27070000767707825\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21709999442100525\n",
      "pred: tensor([-0.2386], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24770000576972961\n",
      "pred: tensor([-0.2373], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2556999921798706\n",
      "pred: tensor([-0.2082], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26100000739097595\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26930001378059387\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22429999709129333\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23960000276565552\n",
      "pred: tensor([-0.2569], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22300000488758087\n",
      "pred: tensor([-0.2352], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24959999322891235\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19679999351501465\n",
      "pred: tensor([-0.2440], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19930000603199005\n",
      "pred: tensor([-0.2358], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24400000274181366\n",
      "pred: tensor([-0.2427], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20180000364780426\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.227400004863739\n",
      "pred: tensor([-0.2218], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24719999730587006\n",
      "pred: tensor([-0.2374], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23720000684261322\n",
      "pred: tensor([-0.2396], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2321999967098236\n",
      "pred: tensor([-0.2269], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26269999146461487\n",
      "pred: tensor([-0.2487], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2199999988079071\n",
      "pred: tensor([-0.2241], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2475000023841858\n",
      "pred: tensor([-0.2291], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24250000715255737\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19009999930858612\n",
      "pred: tensor([-0.2415], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2410999983549118\n",
      "pred: tensor([-0.2331], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27619999647140503\n",
      "pred: tensor([-0.2355], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22509999573230743\n",
      "pred: tensor([-0.2414], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26109999418258667\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23000000417232513\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22699999809265137\n",
      "pred: tensor([-0.2213], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2476000040769577\n",
      "pred: tensor([-0.2418], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2320999950170517\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23589999973773956\n",
      "pred: tensor([-0.2288], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25529998540878296\n",
      "pred: tensor([-0.2412], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24140000343322754\n",
      "pred: tensor([-0.2290], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25859999656677246\n",
      "pred: tensor([-0.2423], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.263700008392334\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23720000684261322\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29100000858306885\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.250900000333786\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2387000024318695\n",
      "pred: tensor([-0.2483], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2563999891281128\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25850000977516174\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2757999897003174\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21369999647140503\n",
      "pred: tensor([-0.2594], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2222999930381775\n",
      "pred: tensor([-0.2219], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25459998846054077\n",
      "pred: tensor([-0.9047], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2574000060558319\n",
      "pred: tensor([-0.2388], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27070000767707825\n",
      "pred: tensor([-0.2436], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21719999611377716\n",
      "pred: tensor([-0.2387], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24570000171661377\n",
      "pred: tensor([-0.1910], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2728999853134155\n",
      "pred: tensor([-0.2036], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451999932527542\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.219200000166893\n",
      "pred: tensor([-0.2412], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.226500004529953\n",
      "pred: tensor([-0.8961], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29580000042915344\n",
      "pred: tensor([-0.2312], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2727000117301941\n",
      "pred: tensor([-0.2206], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2508000135421753\n",
      "pred: tensor([-0.2281], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2337999939918518\n",
      "pred: tensor([-0.2343], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24770000576972961\n",
      "pred: tensor([-0.2157], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2590000033378601\n",
      "pred: tensor([-0.1952], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25540000200271606\n",
      "pred: tensor([-0.2207], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23589999973773956\n",
      "pred: tensor([-0.2155], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25769999623298645\n",
      "pred: tensor([-0.2355], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2467000037431717\n",
      "pred: tensor([-0.2586], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21940000355243683\n",
      "pred: tensor([-0.2405], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20419999957084656\n",
      "pred: tensor([-0.2408], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24150000512599945\n",
      "pred: tensor([-0.2436], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20669999718666077\n",
      "pred: tensor([-0.2439], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26179999113082886\n",
      "pred: tensor([-0.2387], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25110000371932983\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2312999963760376\n",
      "pred: tensor([-0.2517], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.243599995970726\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26750001311302185\n",
      "pred: tensor([-0.2474], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.1931000053882599\n",
      "pred: tensor([-0.2323], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2460000067949295\n",
      "pred: tensor([-0.2384], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2556000053882599\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2565000057220459\n",
      "pred: tensor([-0.2449], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24889999628067017\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21359999477863312\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2651999890804291\n",
      "pred: tensor([-0.2148], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2492000013589859\n",
      "pred: tensor([-0.1898], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24879999458789825\n",
      "pred: tensor([-0.2253], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19460000097751617\n",
      "pred: tensor([-0.2393], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2678000032901764\n",
      "pred: tensor([-0.2299], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23010000586509705\n",
      "pred: tensor([-0.2396], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2329999953508377\n",
      "pred: tensor([-0.2398], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2443999946117401\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2289000004529953\n",
      "pred: tensor([-0.2642], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21899999678134918\n",
      "pred: tensor([-0.2433], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21449999511241913\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24269999563694\n",
      "pred: tensor([-0.2197], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24819999933242798\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22310000658035278\n",
      "pred: tensor([-0.2410], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26350000500679016\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23240000009536743\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24650000035762787\n",
      "pred: tensor([-0.2392], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2526000142097473\n",
      "pred: tensor([-0.2553], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2741999924182892\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2328999936580658\n",
      "pred: tensor([-0.2391], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24650000035762787\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24699999392032623\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22519999742507935\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23399999737739563\n",
      "pred: tensor([-0.2585], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2418999969959259\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23579999804496765\n",
      "pred: tensor([-0.2396], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2619999945163727\n",
      "pred: tensor([-0.2323], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21529999375343323\n",
      "pred: tensor([-0.2602], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26249998807907104\n",
      "pred: tensor([-0.2436], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23229999840259552\n",
      "pred: tensor([-0.2075], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2529999911785126\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21940000355243683\n",
      "pred: tensor([-0.1946], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23440000414848328\n",
      "pred: tensor([-0.2228], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23739999532699585\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23720000684261322\n",
      "pred: tensor([-0.2227], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2443999946117401\n",
      "pred: tensor([-0.2413], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22859999537467957\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23569999635219574\n",
      "pred: tensor([-0.2338], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2493000030517578\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2378000020980835\n",
      "pred: tensor([-0.2276], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2492000013589859\n",
      "pred: tensor([-0.2223], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22509999573230743\n",
      "pred: tensor([-0.2348], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2004999965429306\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25029999017715454\n",
      "pred: tensor([-0.2900], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2953999936580658\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23090000450611115\n",
      "pred: tensor([-0.2202], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22529999911785126\n",
      "pred: tensor([-0.2355], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22259999811649323\n",
      "pred: tensor([-0.2375], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26159998774528503\n",
      "pred: tensor([-0.2593], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23749999701976776\n",
      "pred: tensor([-0.2231], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25529998540878296\n",
      "pred: tensor([-0.2416], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2012999951839447\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22830000519752502\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2290000021457672\n",
      "pred: tensor([-0.2423], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23019999265670776\n",
      "pred: tensor([-0.2319], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.225600004196167\n",
      "pred: tensor([-0.2366], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21709999442100525\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27000001072883606\n",
      "pred: tensor([-0.2302], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2290000021457672\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2345999926328659\n",
      "pred: tensor([-0.2329], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2605000138282776\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2906999886035919\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2312999963760376\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24690000712871552\n",
      "pred: tensor([-0.2187], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25209999084472656\n",
      "pred: tensor([-0.2221], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2718000113964081\n",
      "pred: tensor([-0.2395], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2580], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2581999897956848\n",
      "pred: tensor([-0.2260], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25279998779296875\n",
      "pred: tensor([-0.2534], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2409999966621399\n",
      "pred: tensor([-0.2387], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2337000072002411\n",
      "pred: tensor([-0.2286], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2506999969482422\n",
      "pred: tensor([-0.2445], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22200000286102295\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29679998755455017\n",
      "pred: tensor([-0.2321], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2630999982357025\n",
      "pred: tensor([-0.2526], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25220000743865967\n",
      "pred: tensor([-0.2401], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21690000593662262\n",
      "pred: tensor([-0.2375], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2603999972343445\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24079999327659607\n",
      "pred: tensor([-0.2864], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19750000536441803\n",
      "pred: tensor([-0.2191], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.226500004529953\n",
      "pred: tensor([-0.2532], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2483000010251999\n",
      "pred: tensor([-0.2201], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23849999904632568\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21559999883174896\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.19370000064373016\n",
      "pred: tensor([-0.2357], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.257099986076355\n",
      "pred: tensor([-0.2573], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2529999911785126\n",
      "pred: tensor([-0.2116], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2547000050544739\n",
      "pred: tensor([-0.2323], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23489999771118164\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2578999996185303\n",
      "pred: tensor([-0.2350], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2410999983549118\n",
      "pred: tensor([-0.2382], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2815000116825104\n",
      "pred: tensor([-0.2423], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20200000703334808\n",
      "pred: tensor([-0.2357], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2434999942779541\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2337999939918518\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.218299999833107\n",
      "pred: tensor([-0.2377], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2623000144958496\n",
      "pred: tensor([-0.1838], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24660000205039978\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21170000731945038\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22540000081062317\n",
      "pred: tensor([-0.2393], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23770000040531158\n",
      "pred: tensor([-0.2306], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2768999934196472\n",
      "pred: tensor([-0.2545], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25940001010894775\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22609999775886536\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23489999771118164\n",
      "pred: tensor([-0.2246], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24320000410079956\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23430000245571136\n",
      "pred: tensor([-0.2337], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24969999492168427\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2443999946117401\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22499999403953552\n",
      "pred: tensor([-0.2401], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25940001010894775\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21389999985694885\n",
      "pred: tensor([-0.2654], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.16699999570846558\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.219200000166893\n",
      "pred: tensor([-0.2394], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25540000200271606\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22589999437332153\n",
      "pred: tensor([-0.2390], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21899999678134918\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24899999797344208\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2175000011920929\n",
      "pred: tensor([-0.2656], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.17800000309944153\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22210000455379486\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22110000252723694\n",
      "pred: tensor([-0.2544], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2492000013589859\n",
      "pred: tensor([-0.2634], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.17170000076293945\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22609999775886536\n",
      "pred: tensor([-0.2384], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26010000705718994\n",
      "pred: tensor([-0.2592], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21389999985694885\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22949999570846558\n",
      "pred: tensor([-0.2533], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23729999363422394\n",
      "pred: tensor([-0.2649], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2831000089645386\n",
      "pred: tensor([-0.2350], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24330000579357147\n",
      "pred: tensor([-0.2195], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2526000142097473\n",
      "pred: tensor([-0.2865], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24210000038146973\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24549999833106995\n",
      "pred: tensor([-0.2246], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2678999900817871\n",
      "pred: tensor([-0.2197], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2476000040769577\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24089999496936798\n",
      "pred: tensor([-0.2236], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2222999930381775\n",
      "pred: tensor([-0.2147], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2777999937534332\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2092999964952469\n",
      "pred: tensor([-0.2390], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21299999952316284\n",
      "pred: tensor([-0.2405], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23469999432563782\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23489999771118164\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25940001010894775\n",
      "pred: tensor([-0.2327], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24789999425411224\n",
      "pred: tensor([-0.2385], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23999999463558197\n",
      "pred: tensor([-0.2375], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22759999334812164\n",
      "pred: tensor([-0.2411], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23340000212192535\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24120000004768372\n",
      "pred: tensor([-0.2203], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24369999766349792\n",
      "pred: tensor([-0.2440], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24079999327659607\n",
      "pred: tensor([-0.2448], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23909999430179596\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2671000063419342\n",
      "pred: tensor([-0.2428], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23880000412464142\n",
      "pred: tensor([-0.2526], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2754000127315521\n",
      "pred: tensor([-0.2372], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2743000090122223\n",
      "pred: tensor([-0.2584], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24220000207424164\n",
      "pred: tensor([-0.2198], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23309999704360962\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2337999939918518\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23559999465942383\n",
      "pred: tensor([-0.2267], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25519999861717224\n",
      "pred: tensor([-0.2277], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23360000550746918\n",
      "pred: tensor([-0.2343], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24250000715255737\n",
      "pred: tensor([-0.2220], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2345999926328659\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27390000224113464\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22210000455379486\n",
      "pred: tensor([-0.2365], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23890000581741333\n",
      "pred: tensor([-0.2442], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24199999868869781\n",
      "pred: tensor([-0.2374], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2240999937057495\n",
      "pred: tensor([-0.2528], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23800000548362732\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2721000015735626\n",
      "pred: tensor([-0.2183], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24400000274181366\n",
      "pred: tensor([-0.2281], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2329999953508377\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2305999994277954\n",
      "pred: tensor([-0.2158], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2492000013589859\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2484000027179718\n",
      "pred: tensor([-0.2589], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22280000150203705\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.17829999327659607\n",
      "pred: tensor([-0.2551], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23190000653266907\n",
      "pred: tensor([-0.2213], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2540999948978424\n",
      "pred: tensor([-0.2381], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23929999768733978\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24719999730587006\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2517000138759613\n",
      "pred: tensor([-0.2366], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2410999983549118\n",
      "pred: tensor([-0.2368], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24560000002384186\n",
      "pred: tensor([-0.2534], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23800000548362732\n",
      "pred: tensor([-0.2304], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2583000063896179\n",
      "pred: tensor([-0.2358], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2298000007867813\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21619999408721924\n",
      "pred: tensor([-0.2537], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27140000462532043\n",
      "pred: tensor([-0.2291], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2573], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.27900001406669617\n",
      "pred: tensor([-0.2416], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23999999463558197\n",
      "pred: tensor([-0.2123], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23659999668598175\n",
      "pred: tensor([-0.2376], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23389999568462372\n",
      "pred: tensor([-0.2515], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20340000092983246\n",
      "pred: tensor([-0.2240], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24639999866485596\n",
      "pred: tensor([-0.2591], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22470000386238098\n",
      "pred: tensor([-0.2585], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21240000426769257\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24279999732971191\n",
      "pred: tensor([-0.2335], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24050000309944153\n",
      "pred: tensor([-0.2253], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25440001487731934\n",
      "pred: tensor([-0.2413], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.226500004529953\n",
      "pred: tensor([-0.2363], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2142000049352646\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21469999849796295\n",
      "pred: tensor([-0.2439], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21480000019073486\n",
      "pred: tensor([-0.2356], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23729999363422394\n",
      "pred: tensor([-0.2465], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22660000622272491\n",
      "pred: tensor([-0.2386], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22769999504089355\n",
      "pred: tensor([-0.2347], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25220000743865967\n",
      "pred: tensor([-0.2193], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24120000004768372\n",
      "pred: tensor([-0.2554], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24169999361038208\n",
      "pred: tensor([-0.2413], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24420000612735748\n",
      "pred: tensor([-0.2346], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26440000534057617\n",
      "pred: tensor([-0.2188], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2378000020980835\n",
      "pred: tensor([-0.2428], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2542000114917755\n",
      "pred: tensor([-0.2389], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22689999639987946\n",
      "pred: tensor([-0.2249], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2676999866962433\n",
      "pred: tensor([-0.2376], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2328999936580658\n",
      "pred: tensor([-0.2045], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.18649999797344208\n",
      "pred: tensor([-0.2355], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22120000422000885\n",
      "pred: tensor([-0.2499], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23890000581741333\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2345999926328659\n",
      "pred: tensor([-0.2592], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25699999928474426\n",
      "pred: tensor([-0.2407], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21690000593662262\n",
      "pred: tensor([-0.2380], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21140000224113464\n",
      "pred: tensor([-0.2428], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2752000093460083\n",
      "pred: tensor([-0.2209], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23729999363422394\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451999932527542\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.17970000207424164\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24160000681877136\n",
      "pred: tensor([-0.2411], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2551000118255615\n",
      "pred: tensor([-0.2466], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20280000567436218\n",
      "pred: tensor([-0.2441], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23999999463558197\n",
      "pred: tensor([-0.2213], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24379999935626984\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24289999902248383\n",
      "pred: tensor([-0.2207], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24860000610351562\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23070000112056732\n",
      "pred: tensor([-0.2399], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.226500004529953\n",
      "pred: tensor([-0.2428], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22509999573230743\n",
      "pred: tensor([-0.2288], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24580000340938568\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22869999706745148\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23989999294281006\n",
      "pred: tensor([-0.2273], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23559999465942383\n",
      "pred: tensor([-0.2448], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21299999952316284\n",
      "pred: tensor([-0.2444], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23559999465942383\n",
      "pred: tensor([-0.2381], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24410000443458557\n",
      "pred: tensor([-0.2384], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2460000067949295\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2840000092983246\n",
      "pred: tensor([-0.2448], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24490000307559967\n",
      "pred: tensor([-0.2448], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.29980000853538513\n",
      "pred: tensor([-0.2470], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2754000127315521\n",
      "pred: tensor([-0.2482], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20010000467300415\n",
      "pred: tensor([-0.2326], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23739999532699585\n",
      "pred: tensor([-0.2303], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23980000615119934\n",
      "pred: tensor([-0.2446], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2680000066757202\n",
      "pred: tensor([-0.2799], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25040000677108765\n",
      "pred: tensor([-0.2377], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2379000037908554\n",
      "pred: tensor([-0.2106], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2387000024318695\n",
      "pred: tensor([-0.2301], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24210000038146973\n",
      "pred: tensor([-0.2371], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23360000550746918\n",
      "pred: tensor([-0.2324], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24650000035762787\n",
      "pred: tensor([-0.2482], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.1671999990940094\n",
      "pred: tensor([-0.2281], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22540000081062317\n",
      "pred: tensor([-0.2362], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.266400009393692\n",
      "pred: tensor([-0.2270], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23119999468326569\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2249000072479248\n",
      "pred: tensor([-0.2386], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24250000715255737\n",
      "pred: tensor([-0.2321], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2354000061750412\n",
      "pred: tensor([-0.2360], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2371000051498413\n",
      "pred: tensor([-0.2073], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2442999929189682\n",
      "pred: tensor([-0.2361], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2451000064611435\n",
      "pred: tensor([-0.2498], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23280000686645508\n",
      "pred: tensor([-0.2378], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24500000476837158\n",
      "pred: tensor([-0.2491], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23669999837875366\n",
      "pred: tensor([-0.2287], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24869999289512634\n",
      "pred: tensor([-0.2451], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2563000023365021\n",
      "pred: tensor([-0.2351], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23260000348091125\n",
      "pred: tensor([-0.2410], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2232999950647354\n",
      "pred: tensor([-0.2455], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22689999639987946\n",
      "pred: tensor([-0.2296], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24969999492168427\n",
      "pred: tensor([-0.2413], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20800000429153442\n",
      "pred: tensor([-0.2481], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.249099999666214\n",
      "pred: tensor([-0.1899], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24210000038146973\n",
      "pred: tensor([-0.2333], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2190999984741211\n",
      "pred: tensor([-0.2340], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23909999430179596\n",
      "pred: tensor([-0.2341], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2460000067949295\n",
      "pred: tensor([-0.2683], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26350000500679016\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20440000295639038\n",
      "pred: tensor([-0.2221], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2529999911785126\n",
      "pred: tensor([-0.2086], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.26080000400543213\n",
      "pred: tensor([-0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2393999993801117\n",
      "pred: tensor([-0.2631], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22339999675750732\n",
      "pred: tensor([-0.2515], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24490000307559967\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.22429999709129333\n",
      "pred: tensor([-0.2535], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24060000479221344\n",
      "pred: tensor([-0.2608], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21050000190734863\n",
      "pred: tensor([-0.2349], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.24130000174045563\n",
      "pred: tensor([-0.2461], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.25200000405311584\n",
      "pred: tensor([-0.2418], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2524999976158142\n",
      "pred: tensor([-0.2369], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2678999900817871\n",
      "pred: tensor([-0.2367], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2134999930858612\n",
      "pred: tensor([-0.2291], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.2402999997138977\n",
      "pred: tensor([-0.2540], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.211899995803833\n",
      "pred: tensor([-0.2324], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.23989999294281006\n",
      "pred: tensor([-0.2406], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.20260000228881836\n",
      "pred: tensor([-0.2347], dtype=torch.float64, grad_fn=<AddBackward0>)  target: -0.21400000154972076\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "targets = []\n",
    "for i in range(1000):\n",
    "    pred = model.forward(adj(dataset2[i]), sig(dataset2[i]))\n",
    "    prediction.append(pred)\n",
    "    tar = dataset2[i][2][5]\n",
    "    targets.append(tar)\n",
    "    print(f'pred: {pred}  target: {tar}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(targets[0].dtype)\n",
    "torch.is_tensor(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    plt.xlim([-0.4, -0.1])\n",
    "    plt.ylim([-0.4, -0.1])\n",
    "    plt.scatter(targets,prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('graph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ed53abde219ef6853eedc355635d7c6df7de031d756a54cb96ec05f20e91c61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
